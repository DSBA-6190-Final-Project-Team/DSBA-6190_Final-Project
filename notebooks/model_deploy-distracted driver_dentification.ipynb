{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the preprocessing already locally performed. Training and Validation sets of images and LST files have been developed and placed in the correct structure, on S3. \n",
    "\n",
    "At this point, we can now simply perform the multiclass image classification training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Parameters\n",
    "This step establishes AWS parameters used through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::726963482731:role/sagemaker_execution\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "bucket = \"dsba-6190-final-team-project\"\n",
    "prefix_1 = \"channels\"\n",
    "prefix_file_type = \"rec\"\n",
    "\n",
    "sess_sage = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Sagemaker Model\n",
    "This step imports the latest version of the Amazon Sagemaker Image Classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest\n"
     ]
    }
   ],
   "source": [
    "training_image = get_image_uri(sess_sage.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Two different data sets have been uploaded to S3. One is the complete dataset. The other is a 10% sample of the dataset. The 10% sample is for troubleshooting training and deployment of the Sagemaker Image Classification algorithm.\n",
    "\n",
    "There are only two differences between training the model with the sample or complete dataset:\n",
    "\n",
    "* __Input Location__: We need to point the algorithm to different S3 locations. We will do this with the **prefix_dataset** variable, which will be defined at the beginning of each dataset's notebook section.\n",
    "* __Number of Training Samples__: The number of training samples will be different for the complete and the sample. Thes values are available in the Jupyter Notebook used to split the data and upload to S3.\n",
    "\n",
    "We will define the number of **training** samples for each dataset below. \n",
    "\n",
    "**Note**: *Currently this is a manual process. Future iterations of this process will automate this calculation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples_complete = 15686\n",
    "num_training_samples_10 = 1567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset\n",
    "This section defines the parameters of the dataset. By setting the split prefix and dataset prefix, it will direct the algorithm to the correct training and validation inputs. \n",
    "\n",
    "There are two varables which require definition:\n",
    "\n",
    "1. **Dataset**: The dataset is either the complete dataset, or it is the 10% sample dataset. The 10% sample was created for troubleshooting purposes. Final production will use the complete dataset.\n",
    "2. **Train/Validation Split Method**: Two different methods were developed to split the training data into a training and validation set. See the image processing notebook for more detail.\n",
    " * im2rec: This method was a random split, using the **im2rec.py** tool\n",
    " * split_drivers: This method divided the drivers into a training and validation set. Then, all the images associated with each driver are put into image training and validation sets. Using this method, all of the images associated with a driver are in either the training or validation set. No driver appears in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the inputs for the model:\n",
      "Split Method:\t\t\tsplit_random\n",
      "Dataset:\t\t\tcomplete\n",
      "# of Training Samples:\t\t15686\n"
     ]
    }
   ],
   "source": [
    "# Define Lists and Dictionary\n",
    "list_dataset = [\"complete\", \"sample\",]\n",
    "list_split_method = [\"split_random\", \"split_driver\"]\n",
    "\n",
    "training_sample_dict = {\n",
    "    \"sample-split_random\" : num_training_samples_10,\n",
    "    \"sample-split_driver\": num_training_samples_10, \n",
    "    \"complete-split_random\": num_training_samples_complete,\n",
    "    \"complete-split_driver\": num_training_samples_complete    \n",
    "}\n",
    "\n",
    "# Define Data Inputs\n",
    "prefix_dataset = list_dataset[0] #0 = complete / 1 = sample\n",
    "prefix_split_type = list_split_method[0]  #0 = split_random / 1 = split_drivers\n",
    "\n",
    "# Extract Number of Training Samples\n",
    "key_training_sample = prefix_dataset + \"-\" +prefix_split_type\n",
    "num_training_samples = training_sample_dict[key_training_sample]\n",
    "\n",
    "print(\"The following are the inputs for the model:\")\n",
    "print(\"Split Method:\\t\\t\\t{}\".format(prefix_split_type))\n",
    "print(\"Dataset:\\t\\t\\t{}\".format(prefix_dataset))\n",
    "print(\"# of Training Samples:\\t\\t{}\".format(num_training_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dsba-6190-final-team-project/channels/rec/split_random/complete/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/{}/{}/{}/output'.format(bucket, prefix_1, prefix_split_type, prefix_dataset)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we establish the data input channels. As we are using RecordIO data format, only two channels are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data is pulled from the following S3 locations:\n",
      "Training:\ts3://dsba-6190-final-team-project/channels/split_random/complete/train/\n",
      "Validation:\ts3://dsba-6190-final-team-project/channels/split_random/complete/validation/\n"
     ]
    }
   ],
   "source": [
    "s3train = 's3://{}/{}/{}/{}/train/'.format(bucket, prefix_1, prefix_split_type, prefix_dataset)\n",
    "s3validation = 's3://{}/{}/{}/{}/validation/'.format(bucket, prefix_1, prefix_split_type, prefix_dataset)\n",
    "\n",
    "print(\"The input data is pulled from the following S3 locations:\")\n",
    "print(\"Training:\\t{}\".format(s3train))\n",
    "print(\"Validation:\\t{}\".format(s3validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the channels as inputs into the image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <sagemaker.inputs.s3_input object at 0x7f13586910f0>, 'validation': <sagemaker.inputs.s3_input object at 0x7f1358691048>}\n"
     ]
    }
   ],
   "source": [
    "train_data = sagemaker.session.s3_input(s3train, \n",
    "                                        distribution='FullyReplicated', \n",
    "                                        content_type='application/x-recordio', \n",
    "                                        s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3validation, \n",
    "                                             distribution='FullyReplicated', \n",
    "                                             content_type='application/x-recordio', \n",
    "                                             s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data}\n",
    "\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Instance Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This training session used the following instance: ml.p2.xlarge\n"
     ]
    }
   ],
   "source": [
    "# Available Instances\n",
    "available_instances =['ml.p2.xlarge',              ### $1.26/hr\n",
    "                      'ml.p3.2xlarge'              ### 4.284 /hr\n",
    "                     ]\n",
    "\n",
    "# Initialize Instance\n",
    "train_instance_type = available_instances[0]\n",
    "\n",
    "# Print Check\n",
    "print(\"This training session used the following instance: {}\".format(train_instance_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "#### Parameters\n",
    "The following steps define the algoritm parameters and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_drive_ic = sagemaker.estimator.Estimator(training_image,\n",
    "                                              role, \n",
    "                                              train_instance_count=1, \n",
    "                                              train_instance_type=train_instance_type,\n",
    "                                              train_volume_size = 50,\n",
    "                                              train_max_run = 360000,\n",
    "                                              input_mode= 'File',\n",
    "                                              output_path=s3_output_location,\n",
    "                                              sagemaker_session=sess_sage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_drive_ic.set_hyperparameters(num_layers = 18,\n",
    "                                  use_pretrained_model = 1,\n",
    "                                  image_shape = \"3,210,280\", #RGB Pictures, 210 x 280\n",
    "                                  num_classes = 10,\n",
    "                                  mini_batch_size = 128,\n",
    "                                  epochs = 5,\n",
    "                                  learning_rate = 0.01,\n",
    "                                  num_training_samples = num_training_samples,\n",
    "                                  precision_dtype = 'float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model\n",
    "With the data inputs defined, parameters and hyperparameters initialized, we can run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-21 00:02:27 Starting - Starting the training job...\n",
      "2020-04-21 00:02:29 Starting - Launching requested ML instances......\n",
      "2020-04-21 00:03:37 Starting - Preparing the instances for training.........\n",
      "2020-04-21 00:05:13 Downloading - Downloading input data......\n",
      "2020-04-21 00:05:59 Training - Downloading the training image...\n",
      "2020-04-21 00:06:48 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.01', u'use_pretrained_model': u'1', u'epochs': u'5', u'num_training_samples': u'15686', u'num_layers': u'18', u'image_shape': u'3,210,280', u'mini_batch_size': u'128', u'precision_dtype': u'float16', u'num_classes': u'10'}\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.01', u'epochs': u'5', u'lr_scheduler_factor': 0.1, u'num_layers': u'18', u'precision_dtype': u'float16', u'mini_batch_size': u'128', u'num_classes': u'10', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,210,280', u'gamma': 0.9, u'num_training_samples': u'15686'}\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] multi_label: 0\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] num_layers: 18\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] data type: <type 'numpy.float16'>\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] epochs: 5\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] learning_rate: 0.01\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] num_training_samples: 15686\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] mini_batch_size: 128\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] image_shape: 3,210,280\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] num_classes: 10\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] kv_store: device\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:51 INFO 140609063552832] --------------------\u001b[0m\n",
      "\u001b[34m[00:06:51] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[00:06:52] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:06:52 INFO 140609063552832] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[00:07:01] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:07:30 INFO 140609063552832] Epoch[0] Batch [20]#011Speed: 85.040 samples/sec#011accuracy=0.562872\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:07:52 INFO 140609063552832] Epoch[0] Batch [40]#011Speed: 98.492 samples/sec#011accuracy=0.749238\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:08:14 INFO 140609063552832] Epoch[0] Batch [60]#011Speed: 103.869 samples/sec#011accuracy=0.824155\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:08:36 INFO 140609063552832] Epoch[0] Batch [80]#011Speed: 106.740 samples/sec#011accuracy=0.865066\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:08:58 INFO 140609063552832] Epoch[0] Batch [100]#011Speed: 108.459 samples/sec#011accuracy=0.890316\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:20 INFO 140609063552832] Epoch[0] Batch [120]#011Speed: 109.624 samples/sec#011accuracy=0.906573\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:21 INFO 140609063552832] Epoch[0] Train-accuracy=0.907275\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:21 INFO 140609063552832] Epoch[0] Time cost=141.219\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:33 INFO 140609063552832] Epoch[0] Validation-accuracy=0.995443\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:33 INFO 140609063552832] Storing the best model with validation accuracy: 0.995443\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:34 INFO 140609063552832] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:34 INFO 140609063552832] Saved checkpoint to \"/opt/ml/model/cpu/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:09:56 INFO 140609063552832] Epoch[1] Batch [20]#011Speed: 111.958 samples/sec#011accuracy=0.996280\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:10:19 INFO 140609063552832] Epoch[1] Batch [40]#011Speed: 113.933 samples/sec#011accuracy=0.996761\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:10:41 INFO 140609063552832] Epoch[1] Batch [60]#011Speed: 114.615 samples/sec#011accuracy=0.997054\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:11:03 INFO 140609063552832] Epoch[1] Batch [80]#011Speed: 114.934 samples/sec#011accuracy=0.997685\u001b[0m\n",
      "\u001b[34m[04/21/2020 00:11:25 INFO 140609063552832] Epoch[1] Batch [100]#011Speed: 115.134 samples/sec#011accuracy=0.998066\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_drive_ic.fit(inputs = data_channels, logs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile - AWS Neo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
