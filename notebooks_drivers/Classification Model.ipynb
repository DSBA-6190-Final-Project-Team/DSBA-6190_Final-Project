{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the preprocessing already performed. Training and Validation sets of images and LST files have been developed and placed in the correct structure, on S3. \n",
    "\n",
    "At this point, we can now simply perform the multiclass image classification training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 2 µs, total: 15 µs\n",
      "Wall time: 17.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::726963482731:role/sagemaker_execution\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "bucket = \"dsba-6190-final-team-project\"\n",
    "prefix_1 = \"channels\"\n",
    "prefix_file_type = \"rec\"\n",
    "\n",
    "sess_sage = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Sagemaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest\n"
     ]
    }
   ],
   "source": [
    "training_image = get_image_uri(sess_sage.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Two different data sets have been uploaded to S3. One is the complete dataset. The other is a 10% sample of the dataset. The 10% sample is for troubleshooting the algorithm.\n",
    "\n",
    "There are only two differences between fitting the model with the sample data and the complete dataset:\n",
    "\n",
    "* Input Location: We need to point the algorithm to two different S3 locations. We will do this with the **prefix_dataset** variable, which will be defined at the beginning of each dataset's notebook section.\n",
    "* Number of Training Samples: The number of training samples will be different for the complete and the sample. Thes values are available in the Jupyter Notebook used to split the data and upload to S3. We will define the number of **training** samples for each dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples_complete = 15686\n",
    "num_training_samples_10 = 1567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset\n",
    "This section defines the dataset. By setting the split prefix and dataset prefix, it will direct the algorithm to the correct training and validation inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the dataset inputs into the Image Classification Alogorithm:\n",
      "Split Method:\t\t\tsplit_im2rec\n",
      "Dataset:\t\t\tcomplete\n",
      "# of Training Samples:\t\t15686\n"
     ]
    }
   ],
   "source": [
    "# Define Lists and Dictionary\n",
    "list_dataset = [\"sample\", \"complete\"]\n",
    "list_split_method = [\"split_im2rec\", \"split_drivers\"]\n",
    "\n",
    "training_sample_dict = {\n",
    "    \"sample-split_im2rec\" : 1567,\n",
    "    \"sample-split_drivers\":10, \n",
    "    \"complete-split_im2rec\": 15686,\n",
    "    \"complete-split_drivers\": 15686    \n",
    "}\n",
    "\n",
    "# Define Data Inputs\n",
    "prefix_dataset = list_dataset[1]\n",
    "prefix_split_type = list_split_method[0]\n",
    "\n",
    "# Extract Number of Training Samples\n",
    "key_training_sample = prefix_dataset + \"-\" +prefix_split_type\n",
    "num_training_samples = training_sample_dict[key_training_sample]\n",
    "\n",
    "print(\"The following are the dataset inputs into the Image Classification Alogorithm:\")\n",
    "print(\"Split Method:\\t\\t\\t{}\".format(prefix_split_type))\n",
    "print(\"Dataset:\\t\\t\\t{}\".format(prefix_dataset))\n",
    "print(\"# of Training Samples:\\t\\t{}\".format(num_training_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dsba-6190-final-team-project/channels/rec/split_im2rec/complete/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/{}/{}/{}/output'.format(bucket, prefix_1, prefix_file_type, prefix_split_type, prefix_dataset)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we establish the four channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data is pulled from the following S3 locations:\n",
      "Training:\ts3://dsba-6190-final-team-project/channels/rec/split_im2rec/complete/train/\n",
      "Validation:\ts3://dsba-6190-final-team-project/channels/rec/split_im2rec/complete/validation/\n"
     ]
    }
   ],
   "source": [
    "s3train = 's3://{}/{}/{}/{}/{}/train/'.format(bucket, prefix_1, prefix_file_type, prefix_split_type, prefix_dataset)\n",
    "s3validation = 's3://{}/{}/{}/{}/{}/validation/'.format(bucket, prefix_1, prefix_file_type, prefix_split_type, prefix_dataset)\n",
    "\n",
    "print(\"The input data is pulled from the following S3 locations:\")\n",
    "print(\"Training:\\t{}\".format(s3train))\n",
    "print(\"Validation:\\t{}\".format(s3validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the channels as inputs into the image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <sagemaker.inputs.s3_input object at 0x7f4da0e863c8>, 'validation': <sagemaker.inputs.s3_input object at 0x7f4da0e86470>}\n"
     ]
    }
   ],
   "source": [
    "train_data = sagemaker.session.s3_input(s3train, \n",
    "                                        distribution='FullyReplicated', \n",
    "                                        content_type='application/x-recordio', \n",
    "                                        s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3validation, \n",
    "                                             distribution='FullyReplicated', \n",
    "                                             content_type='application/x-recordio', \n",
    "                                             s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data}\n",
    "\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Price Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see if it might be worth paying for a more expensive instance, I am going got test the two least expensive **Accelerated Computing – Current Generation** instances.\n",
    "\n",
    "**Test on 3/30/20**\n",
    "*Prices:*\n",
    "ml.p2.xlarge: 1.26/hr\n",
    "ml.p3.2xlarge: 4.284/hr\n",
    "\n",
    "The prices listed  are taken at 3/30/20 at 12:57 PM\n",
    "\n",
    "*Algorithm Settings:*\n",
    "\n",
    "* epochs: 2\n",
    "* learning_rate: 0.1\n",
    "* instance count: 1\n",
    "* num layers: 18\n",
    "* mini batch size: 128\n",
    "* use pretrained model: 1\n",
    "\n",
    "**Training Times:**\n",
    "\n",
    "*Split Method: im2rec*\n",
    "\n",
    "Data: 10% Sample\n",
    "\n",
    "ml.p2.xlarge: 101 secs\n",
    "\n",
    "ml.p3.2xlarge: 86 secs\n",
    "\n",
    "Data: Complete\n",
    "\n",
    "ml.p2.xlarge: # secs\n",
    "\n",
    "ml.p3.2xlarge: # secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This training session used the following instance: ml.p2.xlarge\n"
     ]
    }
   ],
   "source": [
    "# Available Instances\n",
    "available_instances =['ml.p2.xlarge',              ### $1.26/hr\n",
    "                      'ml.p3.2xlarge'              ### 4.284 /hr\n",
    "                     ]\n",
    "\n",
    "# Initialize Instance\n",
    "train_instance_type = available_instances[0]\n",
    "\n",
    "# Print Check\n",
    "print(\"This training session used the following instance: {}\".format(train_instance_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_drive_ic = sagemaker.estimator.Estimator(training_image,\n",
    "                                              role, \n",
    "                                              train_instance_count=1, \n",
    "                                              train_instance_type=train_instance_type,\n",
    "                                              train_volume_size = 50,\n",
    "                                              train_max_run = 360000,\n",
    "                                              input_mode= 'File',\n",
    "                                              output_path=s3_output_location,\n",
    "                                              sagemaker_session=sess_sage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_drive_ic.set_hyperparameters(num_layers = 18,\n",
    "                                  use_pretrained_model = 1,\n",
    "                                  image_shape = \"3,210,280\", #RGB Pictures, 210 x 280\n",
    "                                  num_classes = 10,\n",
    "                                  mini_batch_size = 128,\n",
    "                                  epochs = 2,\n",
    "                                  learning_rate = 0.1,\n",
    "                                  num_training_samples = num_training_samples,\n",
    "                                  precision_dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-30 18:45:47 Starting - Starting the training job...\n",
      "2020-03-30 18:45:48 Starting - Launching requested ML instances.........\n",
      "2020-03-30 18:47:20 Starting - Preparing the instances for training.........\n",
      "2020-03-30 18:49:09 Downloading - Downloading input data...\n",
      "2020-03-30 18:49:42 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.1', u'use_pretrained_model': u'1', u'epochs': u'2', u'num_training_samples': u'15686', u'num_layers': u'18', u'image_shape': u'3,210,280', u'mini_batch_size': u'128', u'precision_dtype': u'float32', u'num_classes': u'10'}\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.1', u'epochs': u'2', u'lr_scheduler_factor': 0.1, u'num_layers': u'18', u'precision_dtype': u'float32', u'mini_batch_size': u'128', u'num_classes': u'10', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,210,280', u'gamma': 0.9, u'num_training_samples': u'15686'}\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] multi_label: 0\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] num_layers: 18\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] epochs: 2\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] num_training_samples: 15686\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] mini_batch_size: 128\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] image_shape: 3,210,280\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] num_classes: 10\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] kv_store: device\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:28 INFO 140391108474688] --------------------\u001b[0m\n",
      "\u001b[34m[18:50:28] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2564.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[18:50:28] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2564.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:50:29 INFO 140391108474688] Setting number of threads: 3\u001b[0m\n",
      "\n",
      "2020-03-30 18:50:24 Training - Training image download completed. Training in progress.\u001b[34m[18:50:39] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2564.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:51:10 INFO 140391108474688] Epoch[0] Batch [20]#011Speed: 81.429 samples/sec#011accuracy=0.654390\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:51:33 INFO 140391108474688] Epoch[0] Batch [40]#011Speed: 94.456 samples/sec#011accuracy=0.779916\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:51:56 INFO 140391108474688] Epoch[0] Batch [60]#011Speed: 99.675 samples/sec#011accuracy=0.832992\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:52:19 INFO 140391108474688] Epoch[0] Batch [80]#011Speed: 102.445 samples/sec#011accuracy=0.865451\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:52:42 INFO 140391108474688] Epoch[0] Batch [100]#011Speed: 104.165 samples/sec#011accuracy=0.885829\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:05 INFO 140391108474688] Epoch[0] Batch [120]#011Speed: 105.310 samples/sec#011accuracy=0.900245\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:06 INFO 140391108474688] Epoch[0] Train-accuracy=0.900743\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:06 INFO 140391108474688] Epoch[0] Time cost=147.009\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:19 INFO 140391108474688] Epoch[0] Validation-accuracy=0.943359\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:19 INFO 140391108474688] Storing the best model with validation accuracy: 0.943359\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:19 INFO 140391108474688] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:53:43 INFO 140391108474688] Epoch[1] Batch [20]#011Speed: 107.398 samples/sec#011accuracy=0.982143\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:54:06 INFO 140391108474688] Epoch[1] Batch [40]#011Speed: 109.182 samples/sec#011accuracy=0.983422\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:54:29 INFO 140391108474688] Epoch[1] Batch [60]#011Speed: 109.759 samples/sec#011accuracy=0.984503\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:54:52 INFO 140391108474688] Epoch[1] Batch [80]#011Speed: 110.047 samples/sec#011accuracy=0.987269\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:15 INFO 140391108474688] Epoch[1] Batch [100]#011Speed: 110.245 samples/sec#011accuracy=0.988629\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:38 INFO 140391108474688] Epoch[1] Batch [120]#011Speed: 110.349 samples/sec#011accuracy=0.989669\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:39 INFO 140391108474688] Epoch[1] Train-accuracy=0.989754\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:39 INFO 140391108474688] Epoch[1] Time cost=140.352\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:52 INFO 140391108474688] Epoch[1] Validation-accuracy=0.987946\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:52 INFO 140391108474688] Storing the best model with validation accuracy: 0.987946\u001b[0m\n",
      "\u001b[34m[03/30/2020 18:55:52 INFO 140391108474688] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\n",
      "2020-03-30 18:56:10 Uploading - Uploading generated training model\n",
      "2020-03-30 18:56:10 Completed - Training job completed\n",
      "Training seconds: 421\n",
      "Billable seconds: 421\n",
      "CPU times: user 1.25 s, sys: 56 ms, total: 1.31 s\n",
      "Wall time: 10min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_drive_ic.fit(inputs = data_channels, logs = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
