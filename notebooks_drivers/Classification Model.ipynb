{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the preprocessing already performed. Training and Validation sets of images and LST files have been developed and placed in the correct structure, on S3. \n",
    "\n",
    "At this point, we can now simply perform the multiclass image classification training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 2 µs, total: 14 µs\n",
      "Wall time: 17.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish AWS Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::726963482731:role/sagemaker_execution\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "bucket = \"dsba-6190-final-team-project\"\n",
    "prefix_1 = \"channels\"\n",
    "prefix_file_type = \"rec\"\n",
    "prefix_split_type = \"split_im2rec\"\n",
    "\n",
    "sess_sage = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Sagemaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest\n"
     ]
    }
   ],
   "source": [
    "training_image = get_image_uri(sess_sage.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Two different data sets have been uploaded to S3. One is the complete dataset. The other is a 10% sample of the dataset. The 10% sample is for troubleshooting the algorithm.\n",
    "\n",
    "There are only two differences between fitting the model with the sample data and the complete dataset:\n",
    "\n",
    "* Input Location: We need to point the algorithm to two different S3 locations. We will do this with the **prefix_dataset** variable, which will be defined at the beginning of each dataset's notebook section.\n",
    "* Number of Training Samples: The number of training samples will be different for the complete and the sample. Thes values are available in the Jupyter Notebook used to split the data and upload to S3. We will define the number of **training** samples for each dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples_complete = 15686\n",
    "num_training_samples_10 = 1567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "The following analysis will be for the 10% sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This section trains the image classification model on the sample data, which as 1567 images in its training sample.\n"
     ]
    }
   ],
   "source": [
    "prefix_dataset = \"sample\"\n",
    "num_training_samples = num_training_samples_10\n",
    "\n",
    "print(\"This section trains the image classification model on the {} data, which as {} images in its training sample.\".format(prefix_dataset, num_training_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dsba-6190-final-team-project/channels/rec/split_im2rec/sample/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/{}/{}/{}/output'.format(bucket, prefix_1, prefix_file_type, prefix_split_type, prefix_dataset)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we establish the four channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dsba-6190-final-team-project/channels/rec/split_im2rec/sample/train/\n",
      "s3://dsba-6190-final-team-project/channels/rec/split_im2rec/sample/validation/\n"
     ]
    }
   ],
   "source": [
    "s3train = 's3://{}/{}/{}/{}/{}/train/'.format(bucket, prefix_1, prefix_file_type, prefix_split_type, prefix_dataset)\n",
    "s3validation = 's3://{}/{}/{}/{}/{}/validation/'.format(bucket, prefix_1, prefix_file_type, prefix_split_type, prefix_dataset)\n",
    "\n",
    "print(s3train)\n",
    "print(s3validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the channels as inputs into the image classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <sagemaker.inputs.s3_input object at 0x7f1e1921a550>, 'validation': <sagemaker.inputs.s3_input object at 0x7f1e1921a588>}\n"
     ]
    }
   ],
   "source": [
    "train_data = sagemaker.session.s3_input(s3train, \n",
    "                                        distribution='FullyReplicated', \n",
    "                                        content_type='application/x-recordio', \n",
    "                                        s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3validation, \n",
    "                                             distribution='FullyReplicated', \n",
    "                                             content_type='application/x-recordio', \n",
    "                                             s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data}\n",
    "\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_drive_ic = sagemaker.estimator.Estimator(training_image,\n",
    "                                              role, \n",
    "                                              train_instance_count=1, \n",
    "                                              train_instance_type='ml.p3.2xlarge',\n",
    "                                              train_volume_size = 50,\n",
    "                                              train_max_run = 60,\n",
    "                                              input_mode= 'File',\n",
    "                                              output_path=s3_output_location,\n",
    "                                              sagemaker_session=sess_sage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_drive_ic.set_hyperparameters(num_layers = 18,\n",
    "                                  use_pretrained_model = 1,\n",
    "                                  image_shape = \"3,210,280\", #RGB Pictures, 210 x 280\n",
    "                                  num_classes = 10,\n",
    "                                  mini_batch_size = 128,\n",
    "                                  epochs = 2,\n",
    "                                  learning_rate = 0.1,\n",
    "                                  num_training_samples = num_training_samples,\n",
    "                                  precision_dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-30 15:05:05 Starting - Starting the training job...\n",
      "2020-03-30 15:05:07 Starting - Launching requested ML instances.........\n",
      "2020-03-30 15:06:39 Starting - Preparing the instances for training......\n",
      "2020-03-30 15:07:59 Downloading - Downloading input data\n",
      "2020-03-30 15:07:59 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.1', u'use_pretrained_model': u'1', u'epochs': u'2', u'num_training_samples': u'1567', u'num_layers': u'18', u'image_shape': u'3,210,280', u'mini_batch_size': u'128', u'precision_dtype': u'float32', u'num_classes': u'10'}\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.1', u'epochs': u'2', u'lr_scheduler_factor': 0.1, u'num_layers': u'18', u'precision_dtype': u'float32', u'mini_batch_size': u'128', u'num_classes': u'10', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,210,280', u'gamma': 0.9, u'num_training_samples': u'1567'}\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] multi_label: 0\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] num_layers: 18\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] epochs: 2\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] num_training_samples: 1567\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] mini_batch_size: 128\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] image_shape: 3,210,280\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] num_classes: 10\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] kv_store: device\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] --------------------\u001b[0m\n",
      "\u001b[34m[15:08:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2564.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[15:08:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2564.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:47 INFO 140350643181376] Setting number of threads: 7\u001b[0m\n",
      "\n",
      "2020-03-30 15:09:01 Uploading - Uploading generated training model\u001b[34m[15:08:53] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2564.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:56 INFO 140350643181376] Epoch[0] Train-accuracy=0.603516\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:56 INFO 140350643181376] Epoch[0] Time cost=3.185\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:57 INFO 140350643181376] Epoch[0] Validation-accuracy=0.238281\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:57 INFO 140350643181376] Storing the best model with validation accuracy: 0.238281\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:57 INFO 140350643181376] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:59 INFO 140350643181376] Epoch[1] Train-accuracy=0.914062\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:59 INFO 140350643181376] Epoch[1] Time cost=1.867\u001b[0m\n",
      "\u001b[34m[03/30/2020 15:08:59 INFO 140350643181376] Epoch[1] Validation-accuracy=0.229167\u001b[0m\n",
      "\n",
      "2020-03-30 15:09:12 Completed - Training job completed\n",
      "Training seconds: 86\n",
      "Billable seconds: 86\n",
      "CPU times: user 563 ms, sys: 12.9 ms, total: 576 ms\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_drive_ic.fit(inputs = data_channels, logs = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
